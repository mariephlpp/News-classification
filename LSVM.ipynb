{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50a45ca",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5ca9b",
   "metadata": {},
   "source": [
    "## Deep learning projet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d226d",
   "metadata": {},
   "source": [
    "## Marie PHILIPPE & Claire SERRAZ - M2 D3S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ceb123",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c30886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import some basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a82a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/marie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We import some libraries linked to the text cleaning\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import stem, WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9475727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import some libraries from sklearn for the analysis\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da608f",
   "metadata": {},
   "source": [
    "# 2. Import and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3b9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose the path\n",
    "\n",
    "# Path for Marie\n",
    "os.chdir(\"/Users/marie/Desktop/Cours/S1/DL/Project/Data\")\n",
    "\n",
    "# Path for Claire\n",
    "#os.chdir(\"C:/DESKTOP/Project Fake news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f2f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends embarrassing new year eve m...</td>\n",
       "      <td>donald trump wish american happy new year leav...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke becomes internet joke thr...</td>\n",
       "      <td>friday revealed former milwaukee sheriff david...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsessed even obama name coded website i...</td>\n",
       "      <td>christmas day donald trump announced would bac...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis called donald trump christmas speech</td>\n",
       "      <td>pope francis used annual christmas day message...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>fully committed nato back new u approach afgha...</td>\n",
       "      <td>brussels reuters nato ally tuesday welcomed pr...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>lexisnexis withdrew two product chinese market</td>\n",
       "      <td>london reuters lexisnexis provider legal regul...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>minsk cultural hub becomes authority</td>\n",
       "      <td>minsk reuters shadow disused soviet-era factor...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>vatican upbeat possibility pope francis visiti...</td>\n",
       "      <td>moscow reuters vatican secretary state cardina...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>indonesia buy 1 14 billion worth russian jet</td>\n",
       "      <td>jakarta reuters indonesia buy 11 sukhoi fighte...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0      donald trump sends embarrassing new year eve m...   \n",
       "1      drunk bragging trump staffer started russian c...   \n",
       "2      sheriff david clarke becomes internet joke thr...   \n",
       "3      trump obsessed even obama name coded website i...   \n",
       "4      pope francis called donald trump christmas speech   \n",
       "...                                                  ...   \n",
       "44893  fully committed nato back new u approach afgha...   \n",
       "44894     lexisnexis withdrew two product chinese market   \n",
       "44895               minsk cultural hub becomes authority   \n",
       "44896  vatican upbeat possibility pope francis visiti...   \n",
       "44897       indonesia buy 1 14 billion worth russian jet   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      donald trump wish american happy new year leav...       News   \n",
       "1      house intelligence committee chairman devin nu...       News   \n",
       "2      friday revealed former milwaukee sheriff david...       News   \n",
       "3      christmas day donald trump announced would bac...       News   \n",
       "4      pope francis used annual christmas day message...       News   \n",
       "...                                                  ...        ...   \n",
       "44893  brussels reuters nato ally tuesday welcomed pr...  worldnews   \n",
       "44894  london reuters lexisnexis provider legal regul...  worldnews   \n",
       "44895  minsk reuters shadow disused soviet-era factor...  worldnews   \n",
       "44896  moscow reuters vatican secretary state cardina...  worldnews   \n",
       "44897  jakarta reuters indonesia buy 11 sukhoi fighte...  worldnews   \n",
       "\n",
       "                    date class  \n",
       "0      December 31, 2017  fake  \n",
       "1      December 31, 2017  fake  \n",
       "2      December 30, 2017  fake  \n",
       "3      December 29, 2017  fake  \n",
       "4      December 25, 2017  fake  \n",
       "...                  ...   ...  \n",
       "44893   August 22, 2017   true  \n",
       "44894   August 22, 2017   true  \n",
       "44895   August 22, 2017   true  \n",
       "44896   August 22, 2017   true  \n",
       "44897   August 22, 2017   true  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We import the dataset\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data.astype({\"text\": str, 'title':str})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff56de1",
   "metadata": {},
   "source": [
    "# 3. Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac83892",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af309562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the TF-IDF keys for the observations in the variables text and title\n",
    "number_of_dimensions = 1000\n",
    "\n",
    "# Vector representation of the text\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    # Whether the feature should be made of word or character n-grams\n",
    "    analyzer='word',\n",
    "    # Unigrams: we consider one word by one word\n",
    "    ngram_range=(1, 1),\n",
    "    # Construct a vector the 1000 most used words\n",
    "    max_features=number_of_dimensions,\n",
    "    # Don't take the words that have a frequency higher than 100% \n",
    "    max_df=1.0,\n",
    "    # Don't take the words that appear less than 10 times\n",
    "    min_df=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a024e523",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec911b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our train and test sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    data['text'], \n",
    "    data['class'], \n",
    "    test_size=0.20, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a2b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "X_train = tfidf_vectorizer.transform(X_train).toarray()\n",
    "X_test = tfidf_vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e455730",
   "metadata": {},
   "source": [
    "## LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06c38168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy Score   :  0.9959630268945933\n",
      "Testing accuracy Score :  0.9924276169265034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.99      0.99      0.99      4721\n",
      "        true       0.99      0.99      0.99      4259\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use an LSCV model\n",
    "lsvc = svm.LinearSVC(max_iter=300)\n",
    "\n",
    "# We train the data\n",
    "lsvc.fit(X_train,y_train)\n",
    "\n",
    "# We predict the class on the test set\n",
    "pred_lsvc = lsvc.predict(X_test)\n",
    "\n",
    "# We compute the accuracy score\n",
    "lsvc_accuracy = metrics.accuracy_score(y_test,pred_lsvc)\n",
    "\n",
    "#We print our metrics\n",
    "print(\"Training accuracy Score   : \", lsvc.score(X_train,y_train))\n",
    "print(\"Testing accuracy Score : \", lsvc_accuracy)\n",
    "print(metrics.classification_report(pred_lsvc,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
